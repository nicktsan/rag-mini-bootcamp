{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrievers & retrievers.py\n",
    "In order to retrieve relevant chunks of text from Weaviate, we must first vectorize our query using the same model we created our text chunk embeddings with. We then give that query vector to Weaviate. Weaviate uses the query vector to perform a cosine similarity search for the most similar text chunks.\n",
    "\n",
    "If you've already run your indexer, the below code should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework [7]. Indexing starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similari\n",
      "\n",
      "====================================\n",
      "\n",
      "sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages [15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge [16]. The Memory module leverages the LLM’s memory to guide retrieval, creating an unbounded memory pool that aligns the text more closely with data distribution t\n"
     ]
    }
   ],
   "source": [
    "from cheat_code.common_components.vectorizers import Vectorizer\n",
    "from cheat_code.common_components.wcs_client_adapter import WcsClientAdapter\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "wcs_client_adapter = WcsClientAdapter()\n",
    "\n",
    "query = \"How is Naive RAG different from Advanced RAG?\"\n",
    "query_vector = vectorizer.vectorize_query(query)\n",
    "retrieved_chunks_list = wcs_client_adapter.retrieve(query_vector, k=2)\n",
    "\n",
    "print(retrieved_chunks_list[0][0:1000])\n",
    "print(\"\\n====================================\\n\")\n",
    "print(retrieved_chunks_list[1][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever task #1: implement vectorize_query() in workshop_code/common_components/vectorizers.py\n",
    "This should be easy because it's essentially a simpler version of `vectorize_text_chunks()`, which you've already completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework [7]. Indexing starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similari\n",
      "\n",
      "====================================\n",
      "\n",
      "sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family. The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages [15]. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge [16]. The Memory module leverages the LLM’s memory to guide retrieval, creating an unbounded memory pool that aligns the text more closely with data distribution t\n"
     ]
    }
   ],
   "source": [
    "from workshop_code.common_components.vectorizers import Vectorizer\n",
    "from workshop_code.common_components.wcs_client_adapter import WcsClientAdapter\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "wcs_client_adapter = WcsClientAdapter()\n",
    "\n",
    "query = \"How is Naive RAG different from Advanced RAG?\"\n",
    "query_vector = vectorizer.vectorize_query(query)\n",
    "retrieved_chunks_list = wcs_client_adapter.retrieve(query_vector, k=2)\n",
    "\n",
    "print(retrieved_chunks_list[0][0:1000])\n",
    "print(\"\\n====================================\\n\")\n",
    "print(retrieved_chunks_list[1][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever task #2: understand `retrievers.py`\n",
    "Look over the code of `retrievers.py`. If something doesn't make sense, ask one of us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
